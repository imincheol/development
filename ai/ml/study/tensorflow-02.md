
https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-02-2-linear_regression_feed.py

## Ready 

```
$ cd ~/tensorflow
$ source ./bin/activate

(tensorflow) $ python3 

>>> import tensorflow as tf
>>> tf.__version__
```

## TF Operation

```
>>> x_train = [1, 2, 3]
>>> y_train = [1, 2, 3]

>>> W = tf.Variable(tf.random_normal([1]), name='weight')
>>> b = tf.Variable(tf.random_normal([1]), name='bias')
>>> hypothesis = x_train * W + b

>>> cost = tf.reduce_mean( tf.sqaure(hypothesis - y_train) )

>>> optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
>>> train = optimizer.minimize(cost)

>>> sess = tf.Session()
>>> sess.run(tf.global_variables_initializer())

>>> for step in range(2001):
     sess.run(train)
     if step % 100 == 0:
             print(step, sess.run(cost), sess.run(W), sess.run(b))

0 24.405275 [-1.3699234] [0.19441834]
20 0.3598242 [0.38326335] [0.90751606]
40 0.1290944 [0.5685796] [0.9336368]
60 0.11545316 [0.6037405] [0.8963076]
80 0.10484006 [0.6237808] [0.85480785]
100 0.095217325 [0.6415967] [0.8146944]
120 0.086477846 [0.65845317] [0.77641237]
140 0.07854055 [0.67450583] [0.7399244]
160 0.071331814 [0.68980294] [0.7051508]
180 0.0647847 [0.70438105] [0.6720113]
200 0.05883855 [0.71827406] [0.6404293]
220 0.053438086 [0.73151416] [0.6103315]
240 0.04853337 [0.7441319] [0.5816482]
260 0.044078752 [0.7561569] [0.5543128]
280 0.04003303 [0.76761657] [0.5282622]
300 0.036358617 [0.7785378] [0.5034358]
320 0.03302149 [0.7889457] [0.47977617]
340 0.029990636 [0.7988645] [0.45722845]
360 0.027237982 [0.808317] [0.4357404]
380 0.024737991 [0.8173255] [0.41526222]
400 0.022467444 [0.82591045] [0.39574647]
420 0.020405285 [0.834092] [0.37714782]
440 0.018532412 [0.8418891] [0.35942325]
460 0.01683141 [0.84931976] [0.34253165]
480 0.01528658 [0.85640115] [0.32643393]
500 0.013883509 [0.8631498] [0.31109267]
520 0.012609211 [0.8695812] [0.29647243]
540 0.011451907 [0.8757104] [0.28253937]
560 0.0104007935 [0.8815516] [0.26926103]
580 0.009446164 [0.8871183] [0.2566068]
600 0.008579166 [0.89242333] [0.24454716]
620 0.00779172 [0.89747906] [0.23305432]
640 0.007076565 [0.9022971] [0.22210163]
660 0.006427049 [0.9068888] [0.21166366]
680 0.0058371606 [0.91126466] [0.20171626]
700 0.0053013936 [0.9154349] [0.19223632]
720 0.0048148055 [0.9194092] [0.18320191]
740 0.0043728906 [0.9231966] [0.17459211]
760 0.0039715297 [0.9268061] [0.16638692]
780 0.0036069977 [0.93024606] [0.15856731]
800 0.00327594 [0.9335242] [0.1511152]
820 0.0029752583 [0.9366483] [0.14401332]
840 0.0027021852 [0.9396256] [0.13724524]
860 0.0024541563 [0.942463] [0.13079521]
880 0.0022289127 [0.94516695] [0.12464832]
900 0.0020243272 [0.94774395] [0.11879031]
920 0.0018385338 [0.9501998] [0.11320762]
940 0.0016697841 [0.9525402] [0.10788728]
960 0.0015165176 [0.9547707] [0.10281695]
980 0.0013773274 [0.9568963] [0.09798493]
1000 0.0012509096 [0.958922] [0.09338]
1020 0.001136101 [0.96085256] [0.08899143]
1040 0.0010318218 [0.9626923] [0.08480911]
1060 0.00093711755 [0.9644456] [0.08082341]
1080 0.0008511049 [0.96611655] [0.07702502]
1100 0.00077299046 [0.96770895] [0.07340513]
1120 0.00070204027 [0.96922636] [0.06995541]
1140 0.00063760305 [0.9706727] [0.06666783]
1160 0.00057908153 [0.97205096] [0.0635347]
1180 0.00052593445 [0.9733644] [0.06054883]
1200 0.00047766394 [0.9746162] [0.05770328]
1220 0.00043381823 [0.97580916] [0.05499142]
1240 0.00039400437 [0.97694606] [0.05240703]
1260 0.00035783995 [0.9780295] [0.04994411]
1280 0.00032499462 [0.979062] [0.04759692]
1300 0.0002951642 [0.9800461] [0.04536008]
1320 0.00026807227 [0.98098385] [0.04322829]
1340 0.00024346792 [0.9818775] [0.0411967]
1360 0.00022112291 [0.9827292] [0.03926061]
1380 0.00020082803 [0.98354083] [0.03741551]
1400 0.00018239427 [0.9843144] [0.03565711]
1420 0.00016565471 [0.9850516] [0.03398134]
1440 0.00015044808 [0.9857541] [0.03238432]
1460 0.00013663889 [0.9864236] [0.03086238]
1480 0.0001240972 [0.9870616] [0.02941195]
1500 0.000112708214 [0.9876697] [0.02802968]
1520 0.000102362574 [0.9882491] [0.0267124]
1540 9.2968636e-05 [0.98880136] [0.02545705]
1560 8.443498e-05 [0.98932767] [0.02426066]
1580 7.6686854e-05 [0.98982906] [0.02312058]
1600 6.964805e-05 [0.9903073] [0.02203404]
1620 6.325508e-05 [0.9907627] [0.02099849]
1640 5.744936e-05 [0.9911969] [0.02001162]
1660 5.2176398e-05 [0.9916106] [0.01907114]
1680 4.7387326e-05 [0.9920049] [0.01817484]
1700 4.3037497e-05 [0.9923806] [0.01732068]
1720 3.9087274e-05 [0.99273866] [0.01650665]
1740 3.5499976e-05 [0.99307996] [0.0157309]
1760 3.2241525e-05 [0.99340516] [0.01499162]
1780 2.9282333e-05 [0.9937151] [0.01428708]
1800 2.6594635e-05 [0.99401045] [0.01361563]
1820 2.4153289e-05 [0.9942919] [0.01297575]
1840 2.1936956e-05 [0.9945602] [0.01236594]
1860 1.9923571e-05 [0.9948159] [0.0117848]
1880 1.8094155e-05 [0.99505955] [0.01123093]
1900 1.6433763e-05 [0.9952917] [0.01070309]
1920 1.4925288e-05 [0.99551296] [0.01020007]
1940 1.3555463e-05 [0.9957239] [0.00972069]
1960 1.2311678e-05 [0.99592483] [0.00926384]
1980 1.1181496e-05 [0.99611634] [0.00882847]
2000 1.0155135e-05 [0.99629885] [0.00841358]

```

## Placeholders 

```
X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

for step in range(2001):
    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})
    if step % 20 == 0:
        print(step, cost_val, W_val, b_val)
```
